{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face-Recognition-Using-Transfer-Learning",
      "provenance": [],
      "mount_file_id": "1aQXhDhpopohH4hV_6FVJ8d9w5yUnNT22",
      "authorship_tag": "ABX9TyMeeY4H3Qc/FS1xPDyILDwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVNI1510/Basic-program-on-R-language-/blob/main/Face_Recognition_Using_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ffhtj_J05EH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load HAAR face classifier\n",
        "face_classifier = cv2.CascadeClassifier(r'C:\\Python37\\Projects\\Deep-Learning-Face-Recognition-master\\haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Load functions\n",
        "def face_extractor(img):\n",
        "    # Function detects faces and returns the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "    \n",
        "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(img, 1.3, 5)\n",
        "    \n",
        "    if faces is ():\n",
        "        return None\n",
        "    \n",
        "    # Crop all faces found\n",
        "    for (x,y,w,h) in faces:\n",
        "        x=x-10\n",
        "        y=y-10\n",
        "        cropped_face = img[y:y+h+50, x:x+w+50]\n",
        "\n",
        "    return cropped_face\n",
        "\n",
        "# Initialize Webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "count = 0\n",
        "\n",
        "# Collect 100 samples of your face from webcam input\n",
        "while True:\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    if face_extractor(frame) is not None:\n",
        "        count += 1\n",
        "        face = cv2.resize(face_extractor(frame), (400, 400))\n",
        "        #face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Save file in specified directory with unique name\n",
        "        file_name_path = r'C:\\Python37\\Projects\\Deep-Learning-Face-Recognition-master\\images\\test/' + str(count) + '.jpg'\n",
        "        cv2.imwrite(file_name_path, face)\n",
        "\n",
        "        # Put count on images and display live count\n",
        "        cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "        cv2.imshow('Face Cropper', face)\n",
        "        \n",
        "    else:\n",
        "        print(\"Face not found\")\n",
        "        pass\n",
        "\n",
        "    if cv2.waitKey(1) == 13 or count == 100: #13 is the Enter Key\n",
        "        break\n",
        "        \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()      \n",
        "print(\"Collecting Samples Complete\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face_Recognition_test**"
      ],
      "metadata": {
        "id": "mOo059In17y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "from PIL import Image \n",
        "model=load_model(r'')\n",
        "\n",
        "face_cascade=cv2.CascadeClassifier(r'C:\\Python37\\Projects\\models\\research\\object_detection\\data\\haarcascade_frontalface_alt.xml')\n",
        "\n",
        "def face_extractor(img):\n",
        "    # function detects faces and return the cropped face\n",
        "    # If no face detected, it returns the input image\n",
        "    # gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces=face_cascade.detectMultiScale(img,1.3,5)\n",
        "\n",
        "    if faces is ():\n",
        "        return None\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
        "        cropped_face=img[y:y+h,x:x+w]\n",
        "\n",
        "    return cropped_face\n",
        "\n",
        "\n",
        "video_capture=cv2.VideoCapture(0)\n",
        "while True:\n",
        "    _,frame=video_capture.read()\n",
        "\n",
        "    face=face_extractor(frame)\n",
        "    if type(face) is np.ndarray:\n",
        "        face=cv2.resize(face,(224,224))\n",
        "        im= Image.fromarray(face,'RGB')\n",
        "        img_array=np.array(im)\n",
        "        img_array=np.expand_dims(img_array,axis=0)\n",
        "        pred=model.predict(img_array)\n",
        "        print(pred)\n",
        "\n",
        "        name='None matching'\n",
        "        \n",
        "        if(pred[0][0]>0.3):\n",
        "            name='Ashish'\n",
        "        cv2.putText(frame,name,(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
        "    \n",
        "    else:\n",
        "        cv2.putText(frame,\"No Face Found\",(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
        "\n",
        "    cv2.imshow('Video',frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n",
        "    "
      ],
      "metadata": {
        "id": "5uO-fX7G2QN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Face_Recognition_train**"
      ],
      "metadata": {
        "id": "1CP884-J5RSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "train_path = r'C:\\Python37\\Projects\\Face_Rec_Udemy\\25. Face Recognition\\Ashish_images\\train/'\n",
        "valid_path = r'C:\\Python37\\Projects\\Face_Rec_Udemy\\25. Face Recognition\\Ashish_images\\validation/'\n",
        "\n",
        "# add preprocessing layer to the front of VGG\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# don't train existing weights\n",
        "for layer in vgg.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "\n",
        "  \n",
        "  # useful for getting number of classes\n",
        "#folders = glob('Datasets/Train/*')\n",
        "folders = glob('C:/Python37/Projects/Face_Rec_Udemy/25. Face Recognition/Ashish_images/train/*')\n",
        "\n",
        "  \n",
        "\n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(vgg.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs=vgg.input, outputs=prediction)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(r'C:\\Python37\\Projects\\Face_Rec_Udemy\\25. Face Recognition\\Ashish_images\\train/',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(r'C:\\Python37\\Projects\\Face_Rec_Udemy\\25. Face Recognition\\Ashish_images\\validation/',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "'''r=model.fit_generator(training_set,\n",
        "                         samples_per_epoch = 8000,\n",
        "                         nb_epoch = 5,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 2000)'''\n",
        "\n",
        "# fit the modelS\n",
        "r = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=10,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")\n",
        "# loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='train acc')\n",
        "plt.plot(r.history['val_acc'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')\n",
        "\n",
        "model.save(r'C:\\Python37\\Projects\\Transfer-Learning-master/facefeatures_new_model.h5')"
      ],
      "metadata": {
        "id": "mfw9Zlg55aM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}